# streamlit_app.py
# -*- coding: utf-8 -*-
import os
import re
import io
import json
import time
import streamlit as st
import requests
import urllib3
from urllib.parse import urljoin, unquote, urlparse
from requests.utils import requote_uri
from bs4 import BeautifulSoup

st.set_page_config(page_title="ë©”ê°€ìŠ¤í„°ë”” í¬ë¡¤ëŸ¬ í†µí•©", layout="wide")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

def normalize_paragraphs(raw_text: str) -> str:
    lines = [re.sub(r"\s+", " ", ln).strip() for ln in raw_text.split("\n")]
    text = "\n".join(lines)
    return re.sub(r"\n{3,}", "\n\n", text).strip()

def extract_idx_from_onclick(onclick: str) -> str | None:
    m = re.search(r"\((\d+)\)", onclick or "")
    return m.group(1) if m else None

def filename_from_cd(content_disposition: str | None) -> str | None:
    if not content_disposition:
        return None
    m = re.search(r"filename\*\s*=\s*[^']+'[^']*'([^;]+)", content_disposition, flags=re.I)
    if m:
        try:
            return unquote(m.group(1))
        except Exception:
            return m.group(1)
    m = re.search(r'filename\s*=\s*"?([^";]+)"?', content_disposition, flags=re.I)
    return m.group(1) if m else None

def safe_filename(name: str) -> str:
    for ch in ['/', '\\', ':', '*', '?', '"', '<', '>', '|']:
        name = name.replace(ch, '_')
    return name.strip()

def parse_json_or_empty(s: str):
    s = (s or "").strip()
    if not s:
        return {}
    try:
        return json.loads(s)
    except Exception as e:
        st.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {}

def make_session(headers: dict, verify_ssl: bool):
    if not verify_ssl:
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    sess = requests.Session()
    sess.headers.update(headers or {})
    return sess

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE = "https://www.megastudy.net"
ENDPOINTS = {
    "ì…ì‹œ ë¦¬í¬íŠ¸": {
        "LIST":   f"{BASE}/Entinfo/news/news_list_ax.asp",
        "DETAIL": f"{BASE}/Entinfo/news/news_view_ax.asp",
        "referer": f"{BASE}/Entinfo/news/news_list.asp",
        "params_list":   lambda page: {"page": str(page), "caty": "", "cat2": "", "searchType": "", "searchWord": ""},
        "params_detail": lambda idx, page: {"idx": idx, "caty": "", "cat2": "", "page": str(page), "searchType": "", "searchWord": ""},
    },
    "ì…ì‹œ ë‰´ìŠ¤": {
        "LIST":   f"{BASE}/Entinfo/ipsi_news/news_list_ax.asp",
        "DETAIL": f"{BASE}/Entinfo/ipsi_news/news_view_ax.asp",
        "referer": f"{BASE}/Entinfo/ipsi_news/news_list.asp",
        "params_list":   lambda page: {"page": str(page), "caty": "", "cat2": "", "searchType": "", "searchWord": ""},
        "params_detail": lambda idx, page: {"idx": idx, "caty": "", "cat2": "", "page": str(page), "searchType": "", "searchWord": ""},
    },
    "êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ": {
        "LIST":     f"{BASE}/entinfo/g_archive/list_ax.asp",
        "DETAIL":   f"{BASE}/entinfo/g_archive/view_ax.asp",
        "referer":  f"{BASE}/entinfo/g_archive/list.asp",
        "viewpage": f"{BASE}/entinfo/g_archive/view.asp",  # íŒŒì¼ì„œë²„ Referer
        "params_list":   lambda page: {"page": str(page), "searchType": "", "searchWord": ""},
        "params_detail": lambda idx, page: {"idx": idx, "page": "1", "searchType": "", "searchWord": ""},
    },
}
def build_g_archive_detail_referer(idx: str) -> str:
    return f"{ENDPOINTS['êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ']['viewpage']}?idx={idx}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í¬ë¡¤ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_list_page(sess, cookies, source_name, page, verify_ssl):
    url = ENDPOINTS[source_name]["LIST"]
    params = ENDPOINTS[source_name]["params_list"](page)
    r = sess.get(url, params=params, cookies=cookies, verify=verify_ssl, timeout=20)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    rows, seen = [], set()
    for cell in soup.select(".td_lft"):
        a = cell.select_one(".linkTxt")
        if not a: 
            continue
        idx = extract_idx_from_onclick(a.get("onclick"))
        if not idx or idx in seen:
            continue
        seen.add(idx)
        title = clean_spaces(' '.join(cell.stripped_strings))
        rows.append({"idx": idx, "title": title})
    return rows

def fetch_detail_body_and_soup(sess, cookies, source_name, idx, page_for_context, verify_ssl):
    url = ENDPOINTS[source_name]["DETAIL"]
    params = ENDPOINTS[source_name]["params_detail"](idx, page_for_context)
    r = sess.get(url, params=params, cookies=cookies, verify=verify_ssl, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    parts = []
    for content in soup.select(".viewContents"):
        for bad in content(["script", "style"]):
            bad.decompose()
        raw = content.get_text(separator="\n", strip=True)
        norm = normalize_paragraphs(raw)
        if norm:
            parts.append(norm)
    return "\n\n".join(parts), soup

def find_attachments_from_g_archive(detail_soup):
    urls = []
    for a in detail_soup.select(".commonBoardView--items .viewpage_addfile a[href]"):
        href = a.get("href")
        if href:
            urls.append(urljoin(BASE, href))
    return urls

def download_binary(sess, cookies, url, verify_ssl, referer: str):
    safe_url = requote_uri(url)  # í•œê¸€/ê³µë°± ì¸ì½”ë”©
    headers = dict(sess.headers); headers["Referer"] = referer
    with sess.get(safe_url, cookies=cookies, headers=headers,
                  verify=verify_ssl, timeout=120, stream=True) as resp:
        resp.raise_for_status()
        fname = filename_from_cd(resp.headers.get("Content-Disposition"))
        if not fname:
            path_name = unquote(os.path.basename(urlparse(resp.url).path))
            fname = path_name if path_name else "download.bin"
        fname = safe_filename(fname)
        bio = io.BytesIO()
        for chunk in resp.iter_content(chunk_size=262144):
            if chunk:
                bio.write(chunk)
        bio.seek(0)
        return fname, bio

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¸ì…˜ ìƒíƒœ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
S = st.session_state
for key, val in {
    "results": [],
    "config": {},
    "ran": False,
}.items():
    if key not in S:
        S[key] = val

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.header("ì„¤ì •")
source_name = st.sidebar.selectbox("í¬ë¡¤ ëŒ€ìƒ", ["ì…ì‹œ ë¦¬í¬íŠ¸", "ì…ì‹œ ë‰´ìŠ¤", "êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ"])
start_page = st.sidebar.number_input("ì‹œì‘ í˜ì´ì§€", min_value=1, value=1, step=1)
end_page   = st.sidebar.number_input("ë í˜ì´ì§€", min_value=start_page, value=start_page, step=1)

st.sidebar.markdown("**ì¿ í‚¤ JSON** (ë¹„ì›Œë‘ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬)")
cookies_json = st.sidebar.text_area("cookies", value="", height=120, placeholder='{"CK%5FUSER%5FINFO": "..."}')

default_headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
    "Accept": "text/html, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "Referer": ENDPOINTS[source_name]["referer"],
}
headers_json = st.sidebar.text_area("headers", value=json.dumps(default_headers, ensure_ascii=False, indent=2), height=160)

verify_ssl = st.sidebar.checkbox("SSL ê²€ì¦ ì‚¬ìš©(ê¶Œì¥)", value=False)
delay_sec  = st.sidebar.slider("ìš”ì²­ ê°„ ë”œë ˆì´(ì´ˆ)", 0.0, 2.0, 0.4, 0.1)

# êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ ì „ìš© ì˜µì…˜
prefetch=False; max_prefetch_mb=50; want_download=False; ext_filter=""
if source_name == "êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ":
    st.sidebar.markdown("---"); st.sidebar.subheader("ì²¨ë¶€ ì˜µì…˜")
    prefetch = st.sidebar.checkbox("ë¯¸ë¦¬ ë°›ì•„ë‘ê¸°(ì›í´ë¦­)", value=False)
    max_prefetch_mb = st.sidebar.number_input("ë¯¸ë¦¬ë°›ê¸° ìµœëŒ€ í¬ê¸°(MB)", 10, 500, 50, 10)
    want_download = st.sidebar.checkbox("ì„ íƒ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ í‘œì‹œ", value=True)
    ext_filter    = st.sidebar.text_input("í™•ì¥ì í•„í„°(ì‰¼í‘œ, ë¹„ìš°ë©´ ì „ì²´)", value="zip,pdf")

# ì‹¤í–‰ ë²„íŠ¼
if st.sidebar.button("í¬ë¡¤ ì‹¤í–‰"):
    S.results = []
    S.config = {
        "source_name": source_name,
        "start_page": int(start_page),
        "end_page": int(end_page),
        "cookies": parse_json_or_empty(cookies_json),
        "headers": parse_json_or_empty(headers_json) or default_headers,
        "verify_ssl": bool(verify_ssl),
        "delay_sec": float(delay_sec),
        "prefetch": bool(prefetch),
        "max_prefetch_mb": int(max_prefetch_mb),
        "want_download": bool(want_download),
        "ext_filter": str(ext_filter),
    }
    S.ran = True

if st.sidebar.button("ì´ˆê¸°í™”"):
    S.results = []
    S.config = {}
    S.ran = False
    # ì¤€ë¹„ëœ íŒŒì¼ë„ ëª¨ë‘ ì œê±°
    for k in list(S.keys()):
        if k.startswith(("blob_", "name_")):
            del S[k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œëª©/ìº¡ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cfg = S.config if S.ran else {"source_name": source_name, "start_page": start_page, "end_page": end_page}
st.title("ë©”ê°€ìŠ¤í„°ë”” í¬ë¡¤ëŸ¬ í†µí•©")
st.caption(f"ëŒ€ìƒ: {cfg['source_name']} | í˜ì´ì§€: {cfg['start_page']} ~ {cfg['end_page']}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì½œë°±: ì²¨ë¶€ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prep_file(idx: str, ai: int, url: str):
    """ë²„íŠ¼ ì½œë°±: íŒŒì¼ì„ ë°›ì•„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥. ë¦¬ëŸ° í›„ ë°”ë¡œ download_button í‘œì‹œ"""
    try:
        sess_local = make_session(S.config["headers"], S.config["verify_ssl"])
        ref = build_g_archive_detail_referer(idx) if S.config["source_name"] == "êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ" else ENDPOINTS[S.config["source_name"]]["referer"]
        fname, data = download_binary(sess_local, S.config["cookies"], url, S.config["verify_ssl"], referer=ref)
        S[f"name_{idx}_{ai}"] = fname
        S[f"blob_{idx}_{ai}"] = data.getvalue()
    except Exception as e:
        st.warning(f"ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì‹¤íŒ¨: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•œ ê±´ ë Œë”(ì¦‰ì‹œ ì¶œë ¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_one_row(row: dict, cfg: dict):
    idx, title, body = row["idx"], row["title"], row["body"]
    with st.expander(f"[{idx}] {title}", expanded=False):
        st.markdown(body.replace("\n", "  \n"))

        # êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ: ì²¨ë¶€ ë²„íŠ¼ë“¤
        if cfg["source_name"] == "êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ":
            atts = row.get("attachments") or []
            if not atts:
                st.write("(ì²¨ë¶€ ì—†ìŒ)")
                return
            st.markdown("**ì²¨ë¶€íŒŒì¼:**")
            prefetched = row.get("prefetched") if cfg.get("prefetch") else None

            for ai, au in enumerate(atts):
                key_blob = f"blob_{idx}_{ai}"
                key_name = f"name_{idx}_{ai}"

                # 1) prefetch ì„±ê³µ â†’ ë°”ë¡œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                if cfg.get("prefetch") and prefetched and ai < len(prefetched) and prefetched[ai].get("ok"):
                    fname = S.get(key_name, prefetched[ai]["name"])
                    st.download_button(
                        label=f"ğŸ“¥ {fname}",
                        data=S[key_blob],
                        file_name=fname,
                        mime="application/octet-stream",
                        key=f"dl_{idx}_{ai}"
                    )
                    continue

                # 2) ë¯¸ë¦¬ë°›ê¸° ë¯¸ì‚¬ìš©/ì‹¤íŒ¨ â†’ ì¤€ë¹„ ë²„íŠ¼(ì½œë°±) + ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                cols = st.columns([2, 6])
                with cols[0]:
                    st.button(
                        "ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„",
                        key=f"prep_{idx}_{ai}",
                        on_click=prep_file,
                        args=(idx, ai, au),
                    )
                with cols[1]:
                    if key_blob in S and key_name in S:
                        st.download_button(
                            label=f"ğŸ“¥ {S[key_name]}",
                            data=S[key_blob],
                            file_name=S[key_name],
                            mime="application/octet-stream",
                            key=f"dl_ready_{idx}_{ai}"
                        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í¬ë¡¤ ì‹¤í–‰: í˜ì´ì§€/í•­ëª©ë³„ ì¦‰ì‹œ ë Œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if S.ran and S.config:
    cookies = S.config["cookies"]; headers = S.config["headers"]
    verify_ssl = S.config["verify_ssl"]; delay = S.config["delay_sec"]
    prefetch = S.config.get("prefetch", False); max_mb = S.config.get("max_prefetch_mb", 50)

    sess = make_session(headers, verify_ssl)
    total_pages = S.config["end_page"] - S.config["start_page"] + 1
    pbar = st.progress(0, text="í¬ë¡¤ë§ ì¤‘â€¦"); done_pages = 0

    for page in range(S.config["start_page"], S.config["end_page"] + 1):
        st.write(f"### [page {page}]")
        items = fetch_list_page(sess, cookies, S.config["source_name"], page, verify_ssl)
        if not items:
            st.info("ë°ì´í„° ì—†ìŒ")
            done_pages += 1
            pbar.progress(int(done_pages/max(1,total_pages)*100), text=f"{done_pages}/{total_pages} í˜ì´ì§€ ì™„ë£Œ")
            continue

        for it in items:
            idx, title = it["idx"], it["title"]
            body, soup_detail = fetch_detail_body_and_soup(sess, cookies, S.config["source_name"], idx, page, verify_ssl)
            row = {"idx": idx, "title": title, "body": body}

            # êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ: ì²¨ë¶€/ë¯¸ë¦¬ë°›ê¸°
            if S.config["source_name"] == "êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ":
                attaches = find_attachments_from_g_archive(soup_detail)
                row["attachments"] = attaches
                if prefetch and attaches:
                    limit_bytes = max_mb * 1024 * 1024
                    row["prefetched"] = []
                    for ai, au in enumerate(attaches):
                        try:
                            # prefetch ì‹œ ì„¸ì…˜ì— ì €ì¥ â†’ ë°”ë¡œ ë²„íŠ¼ ë Œë”
                            fname, data = download_binary(sess, cookies, au, verify_ssl, referer=build_g_archive_detail_referer(idx))
                            if data.getbuffer().nbytes > limit_bytes:
                                row["prefetched"].append({"ok": False, "reason": "limit", "name": fname})
                                continue
                            key_blob = f"blob_{idx}_{ai}"; key_name = f"name_{idx}_{ai}"
                            S[key_name] = fname; S[key_blob] = data.getvalue()
                            row["prefetched"].append({"ok": True, "name": fname})
                        except Exception as e:
                            row["prefetched"].append({"ok": False, "reason": str(e)})

            # ì¦‰ì‹œ ë Œë” + ëˆ„ì 
            render_one_row(row, S.config)
            S.results.append(row)
            time.sleep(delay)

        done_pages += 1
        pbar.progress(int(done_pages/max(1,total_pages)*100), text=f"{done_pages}/{total_pages} í˜ì´ì§€ ì™„ë£Œ")
        time.sleep(delay)

    st.success(f"ì´ {len(S.results)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    st.dataframe([{"idx": r["idx"], "title": r["title"]} for r in S.results], use_container_width=True)

    # ì„ íƒ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ (êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ)
    if S.config["source_name"] == "êµìœ¡ê¸°ê´€ë°œí‘œìë£Œ" and S.config.get("want_download", False):
        st.markdown("---"); st.subheader("ì²¨ë¶€íŒŒì¼ ì„ íƒ ë‹¤ìš´ë¡œë“œ")
        with_attach = [r for r in S.results if r.get("attachments")]
        if not with_attach:
            st.info("ì²¨ë¶€ê°€ ìˆëŠ” ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            options = [f"{r['idx']} | {r['title']}" for r in with_attach]
            selected_items = st.multiselect("ë‹¤ìš´ë¡œë“œí•  ê²Œì‹œë¬¼ ì„ íƒ", options, default=[])
            exts = [e.strip().lower() for e in (S.config.get("ext_filter") or "").split(",") if e.strip()]
            if st.button("ì„ íƒ í•­ëª© ë‹¤ìš´ë¡œë“œ ì¤€ë¹„"):
                for opt in selected_items:
                    idx_str = opt.split("|", 1)[0].strip()
                    target = next((r for r in with_attach if str(r["idx"]) == idx_str), None)
                    if not target: 
                        continue
                    attaches = target.get("attachments", [])
                    for ai, au in enumerate(attaches):
                        if exts:
                            path = urlparse(au).path.lower()
                            if not any(path.endswith("." + x) for x in exts):
                                continue
                        try:
                            sess_local = make_session(headers, verify_ssl)
                            fname, data = download_binary(sess_local, cookies, au, verify_ssl, referer=build_g_archive_detail_referer(idx_str))
                            st.download_button(
                                label=f"ğŸ“¥ {fname}",
                                data=data,
                                file_name=fname,
                                mime="application/octet-stream",
                                key=f"bulk_dl_{idx_str}_{ai}"
                            )
                        except Exception as e:
                            st.warning(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
